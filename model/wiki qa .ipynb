{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import nltk\n",
    "from torch.nn import init\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the input from the input file.\n",
    "def input_data(path):\n",
    "    \n",
    "    file=open(path,'r',encoding=\"utf-8\")\n",
    "    train=file.readlines()\n",
    "    file.close()\n",
    "    \n",
    "    context=[]\n",
    "    utter=[]\n",
    "    label=[]\n",
    "    \n",
    "    for i in train:\n",
    "        c,u,l=i.split('\\t')#the question, response and label are tab separated \n",
    "        context.append(c.strip().split())\n",
    "        utter.append(u.strip().split())\n",
    "        label.append(int(l.strip()))\n",
    "    \n",
    "    return context,utter,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create pairings for all the unique words in the corpus with a unique index\n",
    "def word_index(context_train,utter_train,context_test,context_valid,utter_test,utter_valid):\n",
    "    \n",
    "    wordindex={}\n",
    "    k=0\n",
    "    train_words=context_train+utter_train+context_test+context_valid+utter_test+utter_valid\n",
    "    \n",
    "    for sentence in tqdm(train_words):\n",
    "        for word in sentence:\n",
    "            \n",
    "            if word not in wordindex.keys():\n",
    "                wordindex[word]=k\n",
    "                k+=1\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    return wordindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to return maximum length of a sentence in a list of sentences.\n",
    "def max_utt_len(sentences):\n",
    "    \n",
    "    maxi=0\n",
    "    for i in sentences:\n",
    "        maxi=max(len(i),maxi)\n",
    "        \n",
    "    return maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert all the words in sentences with their respective indices which are paired in the word_index function.\n",
    "def wordtoindex(sentences,dictionary):\n",
    "    \n",
    "    for sent in tqdm(sentences):\n",
    "        for i in range(len(sent)):\n",
    "            sent[i]=dictionary[sent[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for padding the sentences and creating a mask for each sentence so that all sentences in a corpus are of same size.\n",
    "def matrix(sentence,max_len):\n",
    "    \n",
    "    mask=[]\n",
    "    \n",
    "    for i in tqdm(range(len(sentence))):\n",
    "        \n",
    "        mask.append([1 for i in range(len(sentence[i]))])\n",
    "        for j in range(max_len-len(sentence[i])):\n",
    "            \n",
    "            sentence[i].append(0)\n",
    "            mask[i].append(0)\n",
    "            \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to map index of a word to its respective glove embedding.\n",
    "def id_to_glove(id_dict,glove_embeds):\n",
    "    \n",
    "    id_to_glove={}\n",
    "    for word,embed in glove_embeds.items():\n",
    "        \n",
    "        if word in id_dict:\n",
    "            id_to_glove[id_dict[word]]=np.array(embed,dtype='float32')\n",
    "            \n",
    "#if any word from the corpus is not present in the glove embedding words then embeddings for those words are randomly initiated\n",
    "    for word,index in id_dict.items():\n",
    "        \n",
    "        if index not in id_to_glove:\n",
    "            vec=np.zeros(50,dtype='float32')\n",
    "            vec[:]=np.random.randn(50)*0.01\n",
    "            id_to_glove[ind]=vec\n",
    "            \n",
    "    return id_to_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_train,utter_train,label_train=input_data(\"../Corpus/WikiQA-train.txt\")\n",
    "context_test,utter_test,label_test=input_data(\"../Corpus/WikiQA-test.txt\")\n",
    "context_valid,utter_valid,label_valid=input_data(\"../Corpus/WikiQA-dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all the responses from the training dataset and dumping them as a pkl file for using it during inference.\n",
    "file=open(\"../Corpus/WikiQA-train.txt\",'r',encoding=\"utf-8\")\n",
    "train=file.readlines()\n",
    "file.close()\n",
    "\n",
    "all_answers=[]\n",
    "for i in train:\n",
    "    all_answers.append(i.split('\\t')[1])\n",
    "    \n",
    "file1=open(\"../Backend/allresponses.pkl\",'ab')\n",
    "pickle.dump(all_answers,file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary, words mapping their respective glove embeddings.\n",
    "file=open(\"../Corpus/glove.6B.50d.txt\",'r',encoding=\"utf-8\")\n",
    "wordembeds=file.readlines()\n",
    "file.close()\n",
    "\n",
    "glove_embeds={}\n",
    "for i in tqdm(wordembeds):\n",
    "    \n",
    "    ls=i.strip().split()\n",
    "    \n",
    "    for i in range(1,len(ls)):\n",
    "        ls[i]=float(ls[i])\n",
    "        \n",
    "    glove_embeds[ls[0]]=ls[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating word to index mappings and dumping for further use in inference\n",
    "word_to_index=word_index(context_train,utter_train,context_test,context_valid,utter_test,utter_valid)\n",
    "\n",
    "file=open(\"../Backend/word_to_index_dictionary.pkl\",'ab')\n",
    "pickle.dump(word_to_index,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rid of long texts\n",
    "for i in range(len(utter_train)):\n",
    "    if len(utter_train[i])>150:\n",
    "        utter_train[i]=utter_train[i][:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting all the words to their respective indices\n",
    "wordtoindex(context_train,word_to_index)\n",
    "wordtoindex(utter_train,word_to_index)\n",
    "wordtoindex(context_test,word_to_index)\n",
    "wordtoindex(utter_test,word_to_index)\n",
    "wordtoindex(context_valid,word_to_index)\n",
    "wordtoindex(utter_valid,word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a id to glove embedding mapping\n",
    "words_id_glove=id_to_glove(word_to_index,glove_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating of mask and padding of sentences to max length.\n",
    "mask_utter_train=matrix(utter_train,max_utt_len(utter_train))\n",
    "mask_sentence_train=matrix(context_train,max_utt_len(context_train))\n",
    "mask_utter_test=matrix(utter_test,max_utt_len(utter_test))\n",
    "mask_sentence_test=matrix(context_test,max_utt_len(context_test))\n",
    "mask_utter_valid=matrix(utter_valid,max_utt_len(utter_valid))\n",
    "mask_sentence_valid=matrix(context_valid,max_utt_len(context_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting training testing and validation sets to numpy arrays\n",
    "context_train=np.array(context_train)\n",
    "utter_test=np.array(utter_test)\n",
    "context_test=np.array(context_test)\n",
    "utter_valid=np.array(utter_valid)\n",
    "context_valid=np.array(context_valid)\n",
    "\n",
    "mask_utter_train=np.array(mask_utter_train)\n",
    "mask_sentence_train=np.array(mask_sentence_train)\n",
    "mask_utter_test=np.array(mask_utter_test)\n",
    "mask_sentence_test=np.array(mask_sentence_test)\n",
    "mask_utter_valid=np.array(mask_utter_valid)\n",
    "mask_sentence_valid=np.array(mask_sentence_valid)\n",
    "\n",
    "label_train=np.array(label_train,dtype='float32')\n",
    "label_test=np.array(label_test,dtype='float32')\n",
    "label_valid=np.array(label_valid,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utter_train=np.array(utter_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining torch custom dataloader.\n",
    "class customdataloader(torch.utils.data.Dataset):\n",
    "    def __init__(self,sent,utter,lab,sent_mask,utter_mask):\n",
    "        self.sent=sent\n",
    "        self.utter=utter\n",
    "        self.lab=lab\n",
    "        self.sent_mask=sent_mask\n",
    "        self.utter_mask=utter_mask\n",
    "    def __len__(self):\n",
    "        return len(self.lab)\n",
    "    def __getitem__(self,idx):\n",
    "        return (self.sent[idx],self.utter[idx],self.lab[idx],self.sent_mask[idx],self.utter_mask[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,Dictionary,word_embedding_length=50):\n",
    "        \n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.Dictionary=Dictionary\n",
    "        self.lenDictionary=len(Dictionary)\n",
    "        self.word_embedding_length=word_embedding_length\n",
    "        self.embedding=nn.Embedding(self.lenDictionary,self.word_embedding_length)\n",
    "        self.lstmBlock=nn.LSTM(self.word_embedding_length,self.word_embedding_length)\n",
    "        self.dropout=nn.Dropout(0.5)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):#for providing weights manually\n",
    "        \n",
    "        embedding_weights=torch.FloatTensor(self.lenDictionary,self.word_embedding_length)\n",
    "        \n",
    "        for idx,glove in self.Dictionary.items():\n",
    "            embedding_weights[idx]=torch.FloatTensor(list(glove))#initializing weights with glve vectors\n",
    "            \n",
    "        self.embedding.weight=nn.Parameter(embedding_weights,requires_grad=True)#Training these weights to reach optimal values\n",
    "        self.embedding=nn.Embedding.from_pretrained(self.embedding.weight)\n",
    "    \n",
    "    def forward(self,sent,mask):\n",
    "        \n",
    "        out_sent=self.forwardLSTM(sent,mask)#lstm layer\n",
    "        return out_sent\n",
    "    \n",
    "    def forwardLSTM(self,utt,mask):\n",
    "        \n",
    "        output=torch.zeros([utt.shape[0],self.word_embedding_length])#initializing a output tensor shape->[bathc_size,50]\n",
    "        \n",
    "        for no,(utti,maski) in enumerate(zip(utt,mask)):\n",
    "            \n",
    "            utti_embed=self.embedding(utti)\n",
    "            numutt=torch.sum(maski)\n",
    "            utti_embed=utti_embed[:numutt].unsqueeze(1)#shape->[number_of_words,1,embedding size=50]\n",
    "            \n",
    "            _,(last_hidden,_)=self.lstmBlock(utti_embed)#last_hidden shape->[1,1,embedding_size=50]\n",
    "            last_hidden=self.dropout(last_hidden[0][0])\n",
    "            output[no]=last_hidden\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,optimizer,epoch):\n",
    "    \n",
    "    model.train()#Preparing the model for training.\n",
    "    \n",
    "    for batchid,(sent,utt,lab,masksent,maskutt) in enumerate(train_loader):#getting the batch\n",
    "        \n",
    "        optimizer.zero_grad()#setting the cummulative gradients to 0.\n",
    "        \n",
    "        output_sent=model(sent,masksent)#forward pass shape->[batch_size,50]\n",
    "        output_utt=model(utt,maskutt)#forward pass shape->[batch_size,50]\n",
    "        \n",
    "        #Finding the MSE loss\n",
    "        loss=0\n",
    "        for i in range(len(output_sent)):\n",
    "            loss+=(output_sent[i].dot(output_utt[i])-lab[i])**2\n",
    "        loss/=len(output_sent)\n",
    "        \n",
    "        loss.backward()#calculating gradients of model\n",
    "        optimizer.step()#updating model parameters\n",
    "        \n",
    "        if batchid % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batchid * len(sent), len(train_loader.dataset),\n",
    "            100. * batchid / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model,valid_loader):\n",
    "    \n",
    "    model.eval()#preparing the model for evaluation\n",
    "    correct=0# variable to store total correct predictions\n",
    "    \n",
    "    with torch.no_grad():#to ensure gradients are not calculated as calculating gradients is not required for testing\n",
    "        \n",
    "        for batchid,(sent,utt,lab,masksent,maskutt) in enumerate(valid_loader):\n",
    "            \n",
    "            output_utt=model(utt,maskutt)#forward pass shape->[batch_size,50]\n",
    "            output_sent=model(sent,masksent)#forward pass shape->[batch_size,50]\n",
    "            \n",
    "            for i in range(len(output_sent)):#calculating number of correct predictions\n",
    "                if int(output_sent[i].dot(output_utt[i])+0.5)==lab[i]:\n",
    "                    correct+=1\n",
    "                    \n",
    "        print(100*correct/len(valid_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(seed_value):\n",
    "    #this function removes randomness and makes everything deterministic\n",
    "    #here we set the seed for torch.cuda,torch,numpy and random.\n",
    "    #torch.cuda.manual_seed_all(seed_value) ,if we are using multi-GPU then we should use this to set the seed.\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    seed(0)#fixing the randomness of the code\n",
    "    \n",
    "     #passing the data into custom data loader\n",
    "    train_data=customdataloader(context_train,utter_train,label_train,mask_sentence_train,mask_utter_train)\n",
    "    test_data=customdataloader(context_test,utter_test,label_test,mask_sentence_test,mask_utter_test)\n",
    "    valid_data=customdataloader(context_valid,utter_valid,label_valid,mask_sentence_valid,mask_utter_valid)\n",
    "    \n",
    "    train_loader=DataLoader(train_data,num_workers=0,batch_size=40,shuffle=False)#getting train data loader\n",
    "    test_loader=DataLoader(test_data,num_workers=0,batch_size=1000,shuffle=False)#getting test data loader\n",
    "    valid_loader=DataLoader(valid_data,num_workers=0,batch_size=1000,shuffle=False)#getting validation data loader\n",
    "    \n",
    "    model=Net(words_id_glove)\n",
    "    optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "    \n",
    "    for epoch in range(1,101):\n",
    "        train(model,train_loader,optimizer,epoch)\n",
    "        validation(model,valid_loader)\n",
    "    validation(model,test_loader)\n",
    "    \n",
    "    #Storing all the response embeddings and dumping them in a pkl file for further use during inference.\n",
    "    answer_embeddings=[]\n",
    "    for (sent, utt, lab, masksent, maskutt) in train_loader:\n",
    "        output=model(utt)\n",
    "        answer_embeddings.apppend(torch.tolist(output))\n",
    "    \n",
    "    file = open(\"../Backend/all_response_embeddings.pkl\", 'ab')\n",
    "    pickle.dump(answer_embeddings,file2)\n",
    "    file.close()\n",
    "\n",
    "    #saving the model and loading it\n",
    "    torch.save(model.state_dict(), \"wikiqafinal.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Net(words_id_glove)\n",
    "model.load_state_dict(torch.load(\"wikiqafinal.pt\"))\n",
    "\n",
    "#saving the model in onnx format\n",
    "dummy1=torch.randint(0,2000,(1,len(context_train[0])))\n",
    "dummy2=torch.randint(0,2000,(1,len(context_train[0])))\n",
    "torch.onnx.export(\n",
    "    model,args=(dummy1,dummy2),f=\"model1.onnx\",verbose=True,opset_version=11,\n",
    "    input_names=['data1','data2'],output_names=['output1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
